# Image Caption Generation

The dataset can be downloaded from https://github.com/jbrownlee/Datasets/releases/tag/Flickr8k. The image caption can be downloaded from https://github.com/ysbecca/flickr8k-custom/tree/main/captions.

The basic principle of our image-to-text model is as pictured in the diagram below, where an Encoder network encodes the input image as a feature vector by providing the output of fully-connected layer of a pre-trained CNN. This pre-trained network has been trained on the complete ImageNet dataset and is thus  


At last, I use two methods including BLEU score and cosine similarity to evaluate prediction caption. I compare two methods regarding their principle theory, strength and weakness. It is found that There is some general weakness among two methods. Firstly, both of them cannot distinguish small word changes. For example, the BLEU and the cosine similarity evaluate the candidate ‘a man and a woman are sitting on a bench’ with 0.7 and 0.6 score respectively in terms of the reference ‘a woman and a man stand outside a store’. Even ‘sit’ and ‘stand’ mean the opposite, penalty included by both methods is too low to give reasonable scores. Secondly, neither the BLEU nor the cosine similarity can differentiate words with similar lexical meaning. The idea behind the BLEU makes it greatly penalizes lexical differences even when predictions might be close in meaning to the references. Analogically, the cosine similarity is unable to deal with word synonyms as well.
Differentially, the BLEU evaluates sentences similarity by counting the matching n-grams compared to references. Using simple algorithm，it has a great benefit as a frequent, speedy and inexpensive method to compute accuracy. Based on human translation references, BLEU also serves as a tool to measure the distinction between human and machine prediction, useful for human judgement. Moreover, compared to the cosine similarity, the BLEU score has an explicit differentiation among all candidates which enables to distinguish between the high-level translation and poor one. However, the disadvantage of BLEU is obvious. The number and the quality of reference translations can highly influence BLEU scores. It might indicate the fact that the more references provided, the more accurate BLEU score might be. Consequently, the method of BLEU requires multiple high quality of references per candidate. Besides, it tends to give short candidates respectively high scores. To be specific, the average BLEU score is 0.38 in long sentences with more than 15 words and 0.51 in short sentences with less than 8 words. It might be because matching words have high percentage in short texts even though they are meaningless articles and predicates like ‘a’, ‘an’, ‘the’, ‘is’, ‘are’, etc.
At the meantime, the cosine similarity aims at calculating the cosine of the angle between two text vectors irrespective of their sizes. It is a useful measurement to compare the whole meaning of two text and apply to high-dimension spaces. And it is the main idea, not the length of candidates, could affect the scores. In this case, even if two sentences have far apart magnitude, they could still have high similarity due to orientation closeness. For example, the cosine score is 0.64 between vectors in terms of a long sentence ‘a boy is standing in a pool getting splashed with water’ and a short sentence ‘a boy in a swimming pool’. Besides, the number of common words would not be used to judge whether texts are similar or not, which makes up for the shortage of the BLEU. However, the disadvantage of the cosine similarity is apparent. The cosine score could also be highly affected by the number and quality of references. A high-quality reference could positively influence the cosine score. For example, the cosine score is 0.9 between two highly similar vectors of the candidate ‘a dog swims in the water’ and the first reference ‘dog swims in the water’. However, it is 0.32 score between the candidate and the second reference ‘a dalmatian breed dog paddles through deep water’. Both candidate describe the same photo. The reason why they influence the result significantly might be that different depressions could change the vector orientation dramatically and excessive description might cause them far apart in high dimension spaces.
It could mistakenly mark the prediction poorly in some circumstances. To be specific, references focusing on different subjects might have significantly different vectors therefore influencing the average cosine scores at last. This situation might happen in the photos with various characters and complicated activities.
